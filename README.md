# ğŸµ MIDI Music Generation with LSTM

## ğŸ“ Overview
This project explores the use of deep learning techniques for generating music using LSTM-based neural networks. The model is trained on MIDI files of classical music and learns to generate harmonic musical sequences based on patterns found in the training data.

## âœ¨ Features
- ğŸ“‚ **MIDI Data Processing**: Extracts and analyzes musical sequences from MIDI files.
- ğŸ¼ **Chord Normalization**: Simplifies and standardizes chord representations.
- ğŸ“Š **Data Visualization**: Plots distributions of composers and chord frequencies.
- ğŸ¤– **LSTM-based Model**: Sequential neural network designed for sequence prediction.
- ğŸ¶ **Music Generation**: Generates new musical compositions based on learned patterns.
- ğŸ”Š **MIDI to Audio Conversion**: Uses FluidSynth to convert generated MIDI files to playable audio.

## âš™ï¸ Installation
### ğŸ“Œ Prerequisites
Ensure you have Python installed and the following dependencies:

```bash
pip install numpy pandas seaborn matplotlib tensorflow keras music21 tqdm fluidsynth
```

## ğŸš€ Usage
### ğŸ“¥ 1. Prepare Dataset
- Place MIDI files in the dataset directory.
- The script collects file paths and organizes them into a Pandas DataFrame.

### ğŸ¼ 2. Preprocess Data
- Normalize note durations using `round_chord_durations`.
- Filter out rare chords to focus on frequently used ones.


ğŸ”§ **Hyperparameters:**
- ğŸ“ `BATCH_SIZE = 32` â€“ Number of sequences processed per step.
- ğŸ” `NUM_EPOCHS = 90` â€“ Total training iterations over the dataset.
- ğŸµ `SEQUENCE_LENGTH = 64` â€“ Number of notes in each input sequence.

## ğŸ—ï¸ 3. Model Architecture
The model consists of:
- ğŸ§  **LSTM Layers** (512 and 256 units) to learn sequential dependencies.
- ğŸš« **Dropout Layers** (0.2) to prevent overfitting.
- ğŸ”¢ **Dense Layers** (256 â†’ 128 â†’ Output) with ReLU activation.
- ğŸ“Š **Softmax Output Layer** to predict the next chord in the sequence.
- âš¡ **Adam Optimizer** with a learning rate of `0.0005`.

## ğŸ“ˆ Results
- âœ… The model successfully generates MIDI compositions that follow the harmonic structures of the training dataset.
- ğŸ“Š Visualization of chord distributions helps in understanding common patterns in classical music.

## ğŸ§ Sample Generated Music
You can listen to a sample generated melody here:
https://github.com/user-attachments/assets/1c138f27-6838-4113-a5ba-c2d1c6c1ceda

yep:
<video src="https://github.com/user-attachments/assets/1c138f27-6838-4113-a5ba-c2d1c6c1ceda" width="300" />


## ğŸ”® Future Improvements
- ğŸ· Train on a more diverse dataset including jazz and pop music.
- ğŸ¯ Implement attention mechanisms for improved sequence modeling.
- ğŸ—ï¸ Experiment with different architectures such as Transformer-based models.

## ğŸ™Œ Acknowledgments
This project utilizes open-source datasets from Kaggle and libraries like TensorFlow, Keras, and Music21.

## ğŸ“œ License
This project is licensed under the MIT License.
